{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Building Support Vector Machine.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SRnAoStzQZ67"
      ],
      "authorship_tag": "ABX9TyP3lcAB9greTFF1qxn7RGN7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Malikumair786/Machine_Learning/blob/main/Building_Support_Vector_Machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Support Vector Machine Model**\n"
      ],
      "metadata": {
        "id": "SRnAoStzQZ67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Supervised learning model (label)\n",
        "2. Both Classification and Regression but mainly used for classification\n",
        "3. binary classification\n",
        "4. hyperplane\n",
        "5. Support vectors\n",
        "\n",
        "Support vector machine tried to find the hyperplane that can seperate the two data accordingly. \n",
        "\n",
        "**Support vector** are the datapoints that are very close tothe hyperplane.\n",
        "\n",
        "\n",
        "**Hyperplane:**\n",
        "\n",
        "Hyperplane is a line(in 2d space) or a plane that seperate the data points into 2 classes.\n",
        "\n",
        "**Support Vactors:**\n",
        "Support vectors are the data points which lie nearest to the hyperplane. If these data points changes, the position of the hyperplane changes.\n",
        "\n",
        "**Margin:**\n",
        "The difference between support vector from one class to otherclass.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "1.  Works well with smaller dataset.\n",
        "2. Work efficiently when there is a clear margin of seperation.\n",
        "3. Works well with high dimensional data.\n",
        "\n",
        "**Disadvantages:**\n",
        "1. Not suitable for larger dataset as the training time is higher.\n",
        "2. Not Suitable for noisier dataset (which contain a lot of outliers) with overlapping classes."
      ],
      "metadata": {
        "id": "Ul1VZSxvQZ2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Math Behind SVM Classifier"
      ],
      "metadata": {
        "id": "2tmdulowQZzJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the margin distance is greater, Our SPM can make good predictions.\n",
        "\n",
        "Kernal is used to increase the dimensional space. for exampleconverting from 2-d to 3-d.\n",
        "\n",
        "\n",
        "margin distance: \n",
        "\n",
        "(x1-x2) = 2/||w||\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4ulvbu1cQZv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SVM Kernals"
      ],
      "metadata": {
        "id": "W76fOk8TQZqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kernal functions generally transforms the training set of data so that a non-linear decision surfacecan be transformed to a linear equation in a higher number of dimensions spaces. It returns the inner product between two points in a standard feature dimensions.\n",
        "\n",
        "TYpes of SVM Kernal:\n",
        "1. Linear Kernal\n",
        "2. Polynomial Kernal\n",
        "3. Radial Basis Function (rbf)\n",
        "4. Sigmoid\n"
      ],
      "metadata": {
        "id": "Jfv7o9_9QZdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Kernal:**\n",
        "k(x1,x2) = x1Tx2\n",
        "\n",
        "Used when the numbe of feature that we have in our dataset in high.\n",
        "\n",
        "**Polynomial Kernal**\n",
        "k(x1,x2) = (x1^tx2+r)^d \n",
        "\n",
        "d = degree\n",
        "\n",
        "Used when the numbe of feature that we have in our dataset in low.\n",
        "\n",
        "\n",
        "**Radia Basis Function:**\n",
        "\n",
        "k(x1,x2) = exp(-y.||x1-x2||^2))\n",
        "\n",
        "used when we have very low number of features. \n",
        "\n",
        "**Sigmoid Kernal:**\n",
        "\n",
        "k(x1,x2) = tanh(y.x1^t.x2+r)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r-Of1c8Dj0dI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loss Function for SVM Classifier"
      ],
      "metadata": {
        "id": "RIvbCUt-zrjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\"Loss function measure how far an estimated value is from its true value.\"\n",
        "The loss function that we have to use in SVM is hinge loss function.\n",
        "\n",
        "\"**Hinge loss** isone of the type of loss function, mainly used for maximum margin classification models.\"\n",
        "\n",
        "Hine loss incorporates a margin distance from the classification boundary into the loss calculation.Even if new observation are classified correctly, they can incur a penalty if the margin from the decision boundary is not large enough.\n",
        "\n",
        "0 --> for correct classification\n",
        "\n",
        "1 --> for wrong classification\n",
        "\n",
        "misclassification --> high loss value\n",
        "\n",
        "correct classification --> minimum loss value"
      ],
      "metadata": {
        "id": "94zhiGiRnaMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient Descent for SVM classifier"
      ],
      "metadata": {
        "id": "hmrn5nOIzuah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minimze the cost function,\n",
        "It will try to change some weight and bias value, so that the cost functions changes.\n",
        "\n",
        "All things done with the help of derivatives.\n",
        "\n",
        "\"It is an optimization algorithm used fot minimizing the cost funtion in various machine learning algorithm. It is used for updating the parameter of the learning model.\"\n",
        "\n",
        "Formula: \n",
        "\n",
        "w2 = w1 - learning rate * dw1\n",
        "b2 = b1 - learning rate * db1\n",
        "\n",
        "dw1 = dj/dw\n",
        "\n",
        "db1 = dj/db\n",
        "\n",
        "**Gradient for SVM classifier:**\n",
        "\n",
        "1. if(Yi.(W.X+b) >= 1):\n",
        "\n",
        "  dj/dw = 2*lamda*w\n",
        "\n",
        "  dj/db = 0\n",
        "\n",
        "2. else (Yi.(w.x+b)<1)\n",
        "\n",
        "  dj/dw = 2*lamda*w - Yi.Xi\n",
        "\n",
        "  dj/db = Yi\n",
        "  "
      ],
      "metadata": {
        "id": "MXUPs1f3zzz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Building SVM classifier**"
      ],
      "metadata": {
        "id": "ozdg_vbLD6m7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Equation of the hyperplne\n",
        "Y =wX+b\n",
        "\n",
        "Gradient Descent:\n",
        "\n",
        "w = w-learningRate*dw\n",
        "\n",
        "b = b-learningRate*db\n",
        "\n"
      ],
      "metadata": {
        "id": "16gYWayKGmKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the dependencies"
      ],
      "metadata": {
        "id": "auZUSWvmHXu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "FLE3VXhbEE9z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine Classifier"
      ],
      "metadata": {
        "id": "q-cS5hP8NcUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SVM_classifier():\n",
        "\n",
        "#initiating the hyperparameter\n",
        "  def __init__(self,learning_rate,no_of_iterations,lambda_parameter):\n",
        "\n",
        "    #initiating the hyperparameter\n",
        "    self.learning_rate = learning_rate\n",
        "    self.no_of_iterations = no_of_iterations\n",
        "    self.lambda_parameter = lambda_parameter\n",
        "  \n",
        "  #fitting the dataset to svm classifier\n",
        "  def fit(self, X, Y):\n",
        "    #m --> number of rows, n--> number of columns, no of input features\n",
        "    self.m,self.n = X.shape\n",
        "\n",
        "    #initiating the weight value and bias value\n",
        "    self.w = np.zeros(self.n)\n",
        "    self.b = 0\n",
        "    self.X =X\n",
        "    self.Y = Y\n",
        "\n",
        "    #implementing gradient descent algorithm for optimization\n",
        "    for i in range(self.no_of_iterations):\n",
        "      self.update_weights()\n",
        "\n",
        "  #function for updating the weights and bias value\n",
        "  def update_weights(self):\n",
        "\n",
        "    #Label Encoding\n",
        "    y_label = np.where(self.y <= 0, -1,1)\n",
        "\n",
        "    #gradient (dw,db)\n",
        "    for index, x_i in enumerate(self.X):\n",
        "\n",
        "      condition = y_label[index] * (np.dot(x_i * self.w) - self.b) >= 1\n",
        "      if (condition == True):\n",
        "        dw = 2 * self.lambda_parameter * self.w\n",
        "        db = 0\n",
        "      else:\n",
        "        dw = 2 * self.lambda_parameter * self.w - np.dot(x_i,y_label[index])\n",
        "        db = y_label[index]\n",
        "\n",
        "    self.w = self.w - self.learning_rate * dw\n",
        "\n",
        "    self.b = self.b - self.learning_rate * db\n",
        "\n",
        "  # predict the label for a given input value\n",
        "  def predict(self,X):\n",
        "    \n",
        "    output = np.dot(X,self.w) - self.b\n",
        "\n",
        "    predicted_labels = np.sign(output)\n",
        "\n",
        "    y_hat = np.where(predicted_labels <= -1, 0, 1)\n",
        "    return y_hat"
      ],
      "metadata": {
        "id": "kZnaH7nSHaMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SVM_classifier(learning_rate = 0.001,no_of_iterations = 1000,lambda_parameter = 0.01)"
      ],
      "metadata": {
        "id": "ALrpdKw7Oz-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_list = [10,20,30,40,50]\n",
        "for i, my_list_i in enumerate(my_list):\n",
        "  print(i,my_list_i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2V74_UvUqWy",
        "outputId": "0d5c4371-c5e5-4335-d10c-59e773fe67c1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 10\n",
            "1 20\n",
            "2 30\n",
            "3 40\n",
            "4 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8lXeqnhdVkaY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}